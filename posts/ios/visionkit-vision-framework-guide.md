---
title: iOS VisionKit ä¸ Vision Framework å®Œå…¨æŒ‡å—ï¼šä»æ–‡æ¡£æ‰«æåˆ° AI è§†è§‰è¯†åˆ«
date: 2026-01-10 14:20:00
description: å…¨é¢æ·±å…¥è®²è§£ iOS VisionKit å’Œ Vision Framework çš„ä½¿ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–‡æ¡£æ‰«æã€OCR æ–‡å­—è¯†åˆ«ã€ç‰©ä½“æ£€æµ‹ã€äººè„¸è¯†åˆ«ã€å›¾åƒåˆ†ç±»ç­‰åŠŸèƒ½ï¼Œé…æœ‰å®Œæ•´çš„ SwiftUI å®æˆ˜æ¡ˆä¾‹å’Œæ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚
keywords:
  - iOS
  - VisionKit
  - Vision Framework
  - OCR
  - å›¾åƒè¯†åˆ«
  - SwiftUI
categories:
  - iOS å¼€å‘
tags:
  - iOS
  - VisionKit
  - Vision Framework
  - å›¾åƒè¯†åˆ«
  - OCR
  - SwiftUI
cover: https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1920
---

# iOS VisionKit ä¸ Vision Framework å®Œå…¨æŒ‡å—

åœ¨ iOS å¼€å‘ä¸­ï¼ŒApple æä¾›äº†å¼ºå¤§çš„è§†è§‰è¯†åˆ«èƒ½åŠ›ã€‚æœ¬æ–‡å°†æ·±å…¥ä»‹ç» **VisionKit** å’Œ **Vision Framework** è¿™ä¸¤ä¸ªæ¡†æ¶ï¼Œå¸®åŠ©ä½ ç†è§£å®ƒä»¬çš„åŒºåˆ«ã€ä½¿ç”¨åœºæ™¯ï¼Œå¹¶é€šè¿‡å®æˆ˜ä»£ç æŒæ¡ä»æ–‡æ¡£æ‰«æåˆ° AI ç‰©ä½“è¯†åˆ«çš„å®Œæ•´å®ç°ã€‚

## iOS ç³»ç»Ÿç‰ˆæœ¬å æœ‰ç‡åˆ†æï¼ˆ2026 å¹´åˆï¼‰

### å½“å‰å¸‚åœºå æœ‰ç‡åˆ†å¸ƒ

æ ¹æ®æœ€æ–°æ•°æ®ï¼ˆ2025å¹´åº•-2026å¹´åˆï¼‰ï¼š

| iOS ç‰ˆæœ¬ | å¸‚åœºå æœ‰ç‡ | å‘å¸ƒæ—¶é—´ | çŠ¶æ€ |
|---------|-----------|---------|------|
| **iOS 18.x** | ~39.57% | 2024å¹´9æœˆ | ä¸»æµç‰ˆæœ¬ |
| **iOS 17.x** | ~25% | 2023å¹´9æœˆ | è¾ƒé«˜å æœ‰ç‡ |
| **iOS 16.x** | ~15% | 2022å¹´9æœˆ | ä¸­ç­‰å æœ‰ç‡ |
| **iOS 15.x** | ~10% | 2021å¹´9æœˆ | é€æ¸å‡å°‘ |
| **iOS 14åŠä»¥ä¸‹** | ~10% | 2020å¹´åŠä¹‹å‰ | å°‘é‡ç”¨æˆ· |

### å…³é”®æ´å¯Ÿ

- **iOS 18+ ç´¯è®¡å æœ‰ç‡çº¦ 40%**
- **iOS 16+ ç´¯è®¡å æœ‰ç‡çº¦ 80%**ï¼ˆVisionKit å®æ—¶æ‰«æåŠŸèƒ½å¯ç”¨ï¼‰
- **iOS 13+ ç´¯è®¡å æœ‰ç‡çº¦ 95%**ï¼ˆVisionKit åŸºç¡€åŠŸèƒ½å¯ç”¨ï¼‰
- **iOS 11+ å‡ ä¹ 100%**ï¼ˆVision Framework å¯ç”¨ï¼‰

### å¼€å‘ç­–ç•¥å»ºè®®

| åº”ç”¨ç±»å‹ | æ¨èæœ€ä½ç‰ˆæœ¬ | åŸå›  | è¦†ç›–ç‡ |
|---------|------------|------|--------|
| **æ–°åº”ç”¨/åˆåˆ›** | iOS 16+ | æœ€ä½³ä½“éªŒï¼Œå‡å°‘æŠ€æœ¯å€ºåŠ¡ | 80% |
| **ä¸»æµæ¶ˆè´¹åº”ç”¨** | iOS 13+ | å¹³è¡¡ä½“éªŒä¸è¦†ç›–ç‡ | 95% |
| **ä¼ä¸šåº”ç”¨** | iOS 11+ | æœ€å¤§å…¼å®¹æ€§ | 100% |
| **å·¥å…·ç±»åº”ç”¨** | iOS 13+ | éœ€è¦ VisionKit UI | 95% |
| **AI åˆ›æ–°åº”ç”¨** | iOS 17+ | éœ€è¦æœ€æ–°ç‰¹æ€§ | 65% |

## VisionKit & Vision Framework å„ç‰ˆæœ¬åŠŸèƒ½å¯¹æ¯”

### åŠŸèƒ½å¯ç”¨æ€§çŸ©é˜µ

| åŠŸèƒ½ç‰¹æ€§ | iOS 11 | iOS 12 | iOS 13 | iOS 14-15 | iOS 16+ | iOS 17+ | è¦†ç›–ç‡ |
|---------|--------|--------|--------|-----------|---------|---------|--------|
| **Vision Framework** |
| æ–‡æœ¬è¯†åˆ« (OCR) | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | ~100% |
| äººè„¸æ£€æµ‹ | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | ~100% |
| æ¡å½¢ç è¯†åˆ« | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | ~100% |
| ç‰©ä½“åˆ†ç±» | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | ~100% |
| å›¾åƒè¿½è¸ª | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | ~100% |
| æ˜¾è‘—æ€§æ£€æµ‹ | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | ~95% |
| **VisionKit** |
| æ–‡æ¡£æ‰«æ | âŒ | âŒ | âœ… | âœ… | âœ… | âœ… | ~95% |
| å®æ—¶æ•°æ®æ‰«æ | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | ~80% |
| Live Text | âŒ | âŒ | âŒ | âŒ | âœ… | âœ… | ~80% |
| ä¸»ä½“æŠ å›¾ | âŒ | âŒ | âŒ | âŒ | âŒ | âœ… | ~65% |

### è¯¦ç»†åŠŸèƒ½è¯´æ˜

#### iOS 11-12 (Vision Framework Only)

**å¯ç”¨åŠŸèƒ½ï¼š**
- `VNRecognizeTextRequest` - æ–‡æœ¬è¯†åˆ«
- `VNDetectFaceRectanglesRequest` - äººè„¸æ£€æµ‹
- `VNDetectBarcodesRequest` - æ¡å½¢ç è¯†åˆ«
- `VNClassifyImageRequest` - å›¾åƒåˆ†ç±»
- `VNTrackObjectRequest` - ç‰©ä½“è¿½è¸ª

**ä¸å¯ç”¨ï¼š**
- æ‰€æœ‰ VisionKit UI ç»„ä»¶
- æ–‡æ¡£æ‰«æå™¨

#### iOS 13-15 (VisionKit åŸºç¡€ç‰ˆ)

**æ–°å¢åŠŸèƒ½ï¼š**
- `VNDocumentCameraViewController` - æ–‡æ¡£æ‰«æ
- æ”¹è¿›çš„æ–‡æœ¬è¯†åˆ«å‡†ç¡®åº¦
- æ˜¾è‘—æ€§åˆ†æ (iOS 13+)

**ä¸å¯ç”¨ï¼š**
- `DataScannerViewController`
- Live Text äº¤äº’
- `ImageAnalysisInteraction`

#### iOS 16+ (VisionKit å®Œæ•´ç‰ˆ)

**æ–°å¢åŠŸèƒ½ï¼š**
- `DataScannerViewController` - å®æ—¶æ‰«æ
- `ImageAnalysisInteraction` - Live Text
- `ImageAnalyzer` - å›¾åƒåˆ†æ
- å®æ—¶æ–‡æœ¬è¯†åˆ«
- å®æ—¶æ¡å½¢ç æ‰«æ

**è¦†ç›–çº¦ 80% çš„æ´»è·ƒç”¨æˆ·**

#### iOS 17+ (æœ€æ–°ç‰¹æ€§)

**æ–°å¢åŠŸèƒ½ï¼š**
- ä¸»ä½“æŠ å›¾ (Subject Lifting)
- `interaction.subject` - æå–å›¾åƒä¸»ä½“
- æ”¹è¿›çš„è¯†åˆ«æ€§èƒ½
- æ›´å¿«çš„å¤„ç†é€Ÿåº¦

**è¦†ç›–çº¦ 65% çš„æ´»è·ƒç”¨æˆ·**

### ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥æœ€ä½³å®è·µ

```swift
import VisionKit

struct FeatureAvailability {

    // æ£€æŸ¥æ˜¯å¦æ”¯æŒå®æ—¶æ•°æ®æ‰«æ
    static var supportsLiveDataScanning: Bool {
        if #available(iOS 16.0, *) {
            return DataScannerViewController.isSupported &&
                   DataScannerViewController.isAvailable
        }
        return false
    }

    // æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–‡æ¡£æ‰«æ
    static var supportsDocumentScanning: Bool {
        if #available(iOS 13.0, *) {
            return VNDocumentCameraViewController.isSupported
        }
        return false
    }

    // æ£€æŸ¥æ˜¯å¦æ”¯æŒ Live Text
    static var supportsLiveText: Bool {
        if #available(iOS 16.0, *) {
            return true
        }
        return false
    }

    // æ£€æŸ¥æ˜¯å¦æ”¯æŒä¸»ä½“æŠ å›¾
    static var supportsSubjectLifting: Bool {
        if #available(iOS 17.0, *) {
            return true
        }
        return false
    }
}

// ä½¿ç”¨ç¤ºä¾‹
class OCRManager {

    func performOCR(completion: @escaping (String?) -> Void) {
        if FeatureAvailability.supportsLiveDataScanning {
            // æ–¹æ¡ˆ A: ä½¿ç”¨å®æ—¶æ‰«æ (iOS 16+, è¦†ç›– 80%)
            useLiveDataScanner(completion: completion)
        } else if FeatureAvailability.supportsDocumentScanning {
            // æ–¹æ¡ˆ B: ä½¿ç”¨æ–‡æ¡£æ‰«æ (iOS 13+, è¦†ç›– 95%)
            useDocumentScanner(completion: completion)
        } else {
            // æ–¹æ¡ˆ C: ä½¿ç”¨çº¯ Vision Framework (iOS 11+, è¦†ç›– 100%)
            useVisionFramework(completion: completion)
        }
    }

    @available(iOS 16.0, *)
    private func useLiveDataScanner(completion: @escaping (String?) -> Void) {
        print("âœ¨ ä½¿ç”¨ iOS 16+ DataScannerViewController")
        // å®ç°å®æ—¶æ‰«æé€»è¾‘
    }

    @available(iOS 13.0, *)
    private func useDocumentScanner(completion: @escaping (String?) -> Void) {
        print("ğŸ“„ ä½¿ç”¨ iOS 13+ VNDocumentCameraViewController")
        // å®ç°æ–‡æ¡£æ‰«æé€»è¾‘
    }

    private func useVisionFramework(completion: @escaping (String?) -> Void) {
        print("ğŸ” ä½¿ç”¨ iOS 11+ Vision Framework")
        // å®ç°åŸºç¡€ OCR é€»è¾‘
    }
}
```

## æ¡†æ¶æ¦‚è¿°

### Vision Framework

**Vision Framework** æ˜¯ Apple åœ¨ iOS 11 å¼•å…¥çš„åº•å±‚è®¡ç®—æœºè§†è§‰æ¡†æ¶ï¼Œæä¾›å¼ºå¤§çš„å›¾åƒåˆ†æèƒ½åŠ›ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸ” å›¾åƒåˆ†æå’Œå¤„ç†
- ğŸ“ æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ï¼ˆOCRï¼‰
- ğŸ‘¤ äººè„¸æ£€æµ‹å’Œç‰¹å¾ç‚¹è¯†åˆ«
- ğŸ• ç‰©ä½“æ£€æµ‹å’Œåˆ†ç±»
- ğŸ“Š æ¡å½¢ç è¯†åˆ«
- ğŸ¯ å›¾åƒå¯¹é½å’Œè¿½è¸ª
- ğŸ–¼ï¸ æ˜¾è‘—æ€§åˆ†æ
- ğŸ¤– æ”¯æŒ Core ML æ¨¡å‹

**é€‚ç”¨åœºæ™¯ï¼š**
- éœ€è¦ç²¾ç»†æ§åˆ¶è¯†åˆ«æµç¨‹
- æ‰¹é‡å›¾åƒå¤„ç†
- è‡ªå®šä¹‰è¯†åˆ«æ¨¡å‹
- é«˜çº§å›¾åƒåˆ†æ

### VisionKit

**VisionKit** æ˜¯ iOS 13 å¼•å…¥çš„é«˜å±‚æ¡†æ¶ï¼Œæä¾›ç°æˆçš„ UI ç»„ä»¶å’Œç”¨æˆ·äº¤äº’ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸ“„ æ–‡æ¡£æ‰«æï¼ˆè‡ªåŠ¨è¾¹ç¼˜æ£€æµ‹ã€é€è§†æ ¡æ­£ï¼‰
- ğŸ“± å®æ—¶æ–‡æœ¬è¯†åˆ«ï¼ˆiOS 16+ï¼‰
- ğŸ” æ¡å½¢ç /äºŒç»´ç æ‰«æï¼ˆiOS 16+ï¼‰
- ğŸ–¼ï¸ Live Textï¼ˆå›¾åƒæ–‡æœ¬äº¤äº’ï¼‰
- âœ‚ï¸ ä¸»ä½“æŠ å›¾ï¼ˆiOS 17+ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- å¿«é€Ÿå®ç°æ ‡å‡†åŠŸèƒ½
- éœ€è¦ç³»ç»Ÿçº§ UI ä½“éªŒ
- æ–‡æ¡£æ‰«æ
- å®æ—¶ç›¸æœºè¯†åˆ«

### ä¸¤è€…å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ä½ çš„åº”ç”¨ä»£ç                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VisionKit   â”‚  â”‚  Vision Framework  â”‚
â”‚ (é«˜å±‚ UI)     â”‚  â”‚  (åº•å±‚ç®—æ³•)         â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Core ML / Core Imageâ”‚
    â”‚  (æœºå™¨å­¦ä¹ /å›¾åƒå¤„ç†)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³ç³»è¯´æ˜ï¼š**
- VisionKit å†…éƒ¨ä½¿ç”¨ Vision Framework
- Vision Framework å¯ä»¥ç‹¬ç«‹ä½¿ç”¨
- ä¸¤è€…å¯ä»¥é…åˆä½¿ç”¨ï¼Œå‘æŒ¥å„è‡ªä¼˜åŠ¿

## Vision Framework è¯¦è§£

### åŸºç¡€æ¶æ„

Vision Framework é‡‡ç”¨è¯·æ±‚-å¤„ç†å™¨æ¨¡å¼ï¼š

```swift
// 1. åˆ›å»ºè¯·æ±‚ï¼ˆRequestï¼‰
let request = VNRecognizeTextRequest { request, error in
    // å¤„ç†ç»“æœ
}

// 2. åˆ›å»ºè¯·æ±‚å¤„ç†å™¨ï¼ˆRequest Handlerï¼‰
let handler = VNImageRequestHandler(cgImage: cgImage)

// 3. æ‰§è¡Œè¯·æ±‚
try? handler.perform([request])
```

### 1. æ–‡æœ¬è¯†åˆ«ï¼ˆOCRï¼‰

#### åŸºç¡€æ–‡æœ¬è¯†åˆ«

```swift
import Vision
import UIKit

class TextRecognitionManager {

    // è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
    func recognizeText(in image: UIImage, completion: @escaping (String?) -> Void) {
        guard let cgImage = image.cgImage else {
            completion(nil)
            return
        }

        // åˆ›å»ºæ–‡æœ¬è¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest { request, error in
            guard let observations = request.results as? [VNRecognizedTextObservation],
                  error == nil else {
                completion(nil)
                return
            }

            // æå–æ‰€æœ‰æ–‡æœ¬
            let recognizedText = observations.compactMap { observation in
                observation.topCandidates(1).first?.string
            }.joined(separator: "\n")

            completion(recognizedText)
        }

        // é…ç½®è¯†åˆ«å‚æ•°
        request.recognitionLevel = .accurate  // æˆ– .fast
        request.recognitionLanguages = ["zh-Hans", "en-US"]  // æ”¯æŒä¸­è‹±æ–‡
        request.usesLanguageCorrection = true  // å¯ç”¨è¯­è¨€æ ¡æ­£

        // åˆ›å»ºè¯·æ±‚å¤„ç†å™¨å¹¶æ‰§è¡Œ
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])
            } catch {
                print("æ–‡æœ¬è¯†åˆ«å¤±è´¥: \(error)")
                completion(nil)
            }
        }
    }

    // è·å–æ–‡æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«ä½ç½®ï¼‰
    func recognizeTextWithBounds(in image: UIImage, completion: @escaping ([TextResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        let request = VNRecognizeTextRequest { request, error in
            guard let observations = request.results as? [VNRecognizedTextObservation],
                  error == nil else {
                completion([])
                return
            }

            let results = observations.compactMap { observation -> TextResult? in
                guard let candidate = observation.topCandidates(1).first else {
                    return nil
                }

                return TextResult(
                    text: candidate.string,
                    confidence: candidate.confidence,
                    boundingBox: observation.boundingBox
                )
            }

            completion(results)
        }

        request.recognitionLevel = .accurate

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }
}

// æ–‡æœ¬è¯†åˆ«ç»“æœ
struct TextResult {
    let text: String
    let confidence: Float  // ç½®ä¿¡åº¦ 0-1
    let boundingBox: CGRect  // æ–‡æœ¬åœ¨å›¾ç‰‡ä¸­çš„ä½ç½®ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
}

// ä½¿ç”¨ç¤ºä¾‹
let manager = TextRecognitionManager()

manager.recognizeText(in: image) { text in
    if let text = text {
        print("è¯†åˆ«åˆ°çš„æ–‡å­—ï¼š\n\(text)")
    }
}
```

#### å®æ—¶æ–‡æœ¬è¯†åˆ«ï¼ˆç›¸æœºï¼‰

```swift
import AVFoundation
import Vision

class LiveTextRecognitionViewController: UIViewController {

    private var captureSession: AVCaptureSession!
    private var previewLayer: AVCaptureVideoPreviewLayer!

    override func viewDidLoad() {
        super.viewDidLoad()
        setupCamera()
    }

    private func setupCamera() {
        captureSession = AVCaptureSession()
        captureSession.sessionPreset = .high

        // é…ç½®æ‘„åƒå¤´è¾“å…¥
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
              let input = try? AVCaptureDeviceInput(device: camera) else {
            return
        }

        captureSession.addInput(input)

        // é…ç½®è§†é¢‘è¾“å‡º
        let videoOutput = AVCaptureVideoDataOutput()
        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
        captureSession.addOutput(videoOutput)

        // æ·»åŠ é¢„è§ˆå±‚
        previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
        previewLayer.videoGravity = .resizeAspectFill
        previewLayer.frame = view.bounds
        view.layer.addSublayer(previewLayer)

        // å¯åŠ¨ä¼šè¯
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.captureSession.startRunning()
        }
    }
}

extension LiveTextRecognitionViewController: AVCaptureVideoDataOutputSampleBufferDelegate {

    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {

        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {
            return
        }

        // åˆ›å»ºæ–‡æœ¬è¯†åˆ«è¯·æ±‚
        let request = VNRecognizeTextRequest { request, error in
            guard let observations = request.results as? [VNRecognizedTextObservation] else {
                return
            }

            let recognizedText = observations.compactMap {
                $0.topCandidates(1).first?.string
            }

            DispatchQueue.main.async {
                // æ›´æ–° UI æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                self.displayRecognizedText(recognizedText)
            }
        }

        request.recognitionLevel = .fast  // å®æ—¶è¯†åˆ«ä½¿ç”¨å¿«é€Ÿæ¨¡å¼

        // åˆ›å»ºè¯·æ±‚å¤„ç†å™¨
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])

        do {
            try handler.perform([request])
        } catch {
            print("è¯†åˆ«å¤±è´¥: \(error)")
        }
    }

    private func displayRecognizedText(_ texts: [String]) {
        // æ˜¾ç¤ºè¯†åˆ«ç»“æœ
        print("å®æ—¶è¯†åˆ«: \(texts.joined(separator: ", "))")
    }
}
```

### 2. äººè„¸æ£€æµ‹å’Œè¯†åˆ«

```swift
import Vision

class FaceDetectionManager {

    // æ£€æµ‹äººè„¸
    func detectFaces(in image: UIImage, completion: @escaping ([FaceResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        // åˆ›å»ºäººè„¸æ£€æµ‹è¯·æ±‚
        let request = VNDetectFaceRectanglesRequest { request, error in
            guard let observations = request.results as? [VNFaceObservation],
                  error == nil else {
                completion([])
                return
            }

            let faces = observations.map { observation in
                FaceResult(
                    boundingBox: observation.boundingBox,
                    confidence: observation.confidence,
                    roll: observation.roll,
                    yaw: observation.yaw
                )
            }

            completion(faces)
        }

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }

    // æ£€æµ‹äººè„¸ç‰¹å¾ç‚¹ï¼ˆçœ¼ç›ã€é¼»å­ã€å˜´å·´ç­‰ï¼‰
    func detectFaceLandmarks(in image: UIImage, completion: @escaping ([FaceLandmarksResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        let request = VNDetectFaceLandmarksRequest { request, error in
            guard let observations = request.results as? [VNFaceObservation],
                  error == nil else {
                completion([])
                return
            }

            let results = observations.compactMap { observation -> FaceLandmarksResult? in
                guard let landmarks = observation.landmarks else {
                    return nil
                }

                return FaceLandmarksResult(
                    boundingBox: observation.boundingBox,
                    leftEye: landmarks.leftEye?.normalizedPoints ?? [],
                    rightEye: landmarks.rightEye?.normalizedPoints ?? [],
                    nose: landmarks.nose?.normalizedPoints ?? [],
                    outerLips: landmarks.outerLips?.normalizedPoints ?? [],
                    innerLips: landmarks.innerLips?.normalizedPoints ?? []
                )
            }

            completion(results)
        }

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }
}

struct FaceResult {
    let boundingBox: CGRect
    let confidence: Float
    let roll: NSNumber?  // ä¾§å€¾è§’åº¦
    let yaw: NSNumber?   // åèˆªè§’åº¦
}

struct FaceLandmarksResult {
    let boundingBox: CGRect
    let leftEye: [CGPoint]
    let rightEye: [CGPoint]
    let nose: [CGPoint]
    let outerLips: [CGPoint]
    let innerLips: [CGPoint]
}
```

### 3. ç‰©ä½“æ£€æµ‹å’Œåˆ†ç±»

```swift
import Vision
import CoreML

class ObjectDetectionManager {

    // ä½¿ç”¨å†…ç½®åˆ†ç±»å™¨è¯†åˆ«ç‰©ä½“
    func classifyObject(in image: UIImage, completion: @escaping ([ClassificationResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        // åˆ›å»ºå›¾åƒåˆ†ç±»è¯·æ±‚
        let request = VNClassifyImageRequest { request, error in
            guard let observations = request.results as? [VNClassificationObservation],
                  error == nil else {
                completion([])
                return
            }

            // ç­›é€‰ç½®ä¿¡åº¦é«˜äº 0.5 çš„ç»“æœ
            let results = observations
                .filter { $0.confidence > 0.5 }
                .map { observation in
                    ClassificationResult(
                        identifier: observation.identifier,
                        confidence: observation.confidence
                    )
                }

            completion(results)
        }

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }

    // ä½¿ç”¨è‡ªå®šä¹‰ Core ML æ¨¡å‹
    func detectObjectsWithCustomModel(in image: UIImage, modelURL: URL, completion: @escaping ([DetectionResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        do {
            // åŠ è½½ Core ML æ¨¡å‹
            let model = try VNCoreMLModel(for: MLModel(contentsOf: modelURL))

            // åˆ›å»º Core ML è¯·æ±‚
            let request = VNCoreMLRequest(model: model) { request, error in
                guard let observations = request.results as? [VNRecognizedObjectObservation],
                      error == nil else {
                    completion([])
                    return
                }

                let results = observations.map { observation in
                    DetectionResult(
                        label: observation.labels.first?.identifier ?? "Unknown",
                        confidence: observation.confidence,
                        boundingBox: observation.boundingBox
                    )
                }

                completion(results)
            }

            request.imageCropAndScaleOption = .scaleFit

            let handler = VNImageRequestHandler(cgImage: cgImage)

            DispatchQueue.global(qos: .userInitiated).async {
                try? handler.perform([request])
            }

        } catch {
            print("åŠ è½½æ¨¡å‹å¤±è´¥: \(error)")
            completion([])
        }
    }

    // æ£€æµ‹æ˜¾è‘—æ€§åŒºåŸŸï¼ˆå›¾ç‰‡ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼‰
    func detectSaliency(in image: UIImage, completion: @escaping ([CGRect]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        let request = VNGenerateAttentionBasedSaliencyImageRequest { request, error in
            guard let observation = request.results?.first as? VNSaliencyImageObservation,
                  error == nil else {
                completion([])
                return
            }

            // è·å–æ˜¾è‘—æ€§å¯¹è±¡çš„è¾¹ç•Œæ¡†
            let salientObjects = observation.salientObjects?.map { $0.boundingBox } ?? []
            completion(salientObjects)
        }

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }
}

struct ClassificationResult {
    let identifier: String  // ç‰©ä½“ç±»åˆ«ï¼ˆå¦‚ "dog", "cat"ï¼‰
    let confidence: Float   // ç½®ä¿¡åº¦
}

struct DetectionResult {
    let label: String
    let confidence: Float
    let boundingBox: CGRect
}
```

### 4. æ¡å½¢ç è¯†åˆ«

```swift
import Vision

class BarcodeDetectionManager {

    func detectBarcodes(in image: UIImage, completion: @escaping ([BarcodeResult]) -> Void) {
        guard let cgImage = image.cgImage else {
            completion([])
            return
        }

        // åˆ›å»ºæ¡å½¢ç æ£€æµ‹è¯·æ±‚
        let request = VNDetectBarcodesRequest { request, error in
            guard let observations = request.results as? [VNBarcodeObservation],
                  error == nil else {
                completion([])
                return
            }

            let results = observations.compactMap { observation -> BarcodeResult? in
                guard let payload = observation.payloadStringValue else {
                    return nil
                }

                return BarcodeResult(
                    payload: payload,
                    symbology: observation.symbology.rawValue,
                    boundingBox: observation.boundingBox,
                    confidence: observation.confidence
                )
            }

            completion(results)
        }

        // æŒ‡å®šè¦æ£€æµ‹çš„æ¡å½¢ç ç±»å‹
        request.symbologies = [
            .qr,           // äºŒç»´ç 
            .ean13,        // EAN-13
            .code128,      // Code 128
            .aztec,        // Aztec
            .dataMatrix    // Data Matrix
        ]

        let handler = VNImageRequestHandler(cgImage: cgImage)

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request])
        }
    }
}

struct BarcodeResult {
    let payload: String          // æ¡å½¢ç å†…å®¹
    let symbology: String        // æ¡å½¢ç ç±»å‹
    let boundingBox: CGRect      // ä½ç½®
    let confidence: Float        // ç½®ä¿¡åº¦
}
```

### 5. å›¾åƒå¯¹é½å’Œè¿½è¸ª

```swift
import Vision

class ImageTrackingManager {

    private var lastObservation: VNDetectedObjectObservation?

    // å¼€å§‹è¿½è¸ªæŒ‡å®šåŒºåŸŸ
    func startTracking(object boundingBox: CGRect, in image: UIImage) {
        guard let cgImage = image.cgImage else { return }

        let request = VNDetectRectanglesRequest { request, error in
            guard let observation = request.results?.first as? VNDetectedObjectObservation else {
                return
            }

            self.lastObservation = observation
        }

        let handler = VNImageRequestHandler(cgImage: cgImage)
        try? handler.perform([request])
    }

    // åœ¨æ–°å¸§ä¸­è¿½è¸ªç‰©ä½“
    func trackObject(in image: UIImage, completion: @escaping (CGRect?) -> Void) {
        guard let cgImage = image.cgImage,
              let lastObservation = lastObservation else {
            completion(nil)
            return
        }

        let request = VNTrackObjectRequest(detectedObjectObservation: lastObservation) { request, error in
            guard let observation = request.results?.first as? VNDetectedObjectObservation,
                  error == nil else {
                completion(nil)
                return
            }

            self.lastObservation = observation
            completion(observation.boundingBox)
        }

        request.trackingLevel = .accurate

        let handler = VNSequenceRequestHandler()

        DispatchQueue.global(qos: .userInitiated).async {
            try? handler.perform([request], on: cgImage)
        }
    }
}
```

## VisionKit è¯¦è§£

### 1. æ–‡æ¡£æ‰«æï¼ˆVNDocumentCameraViewControllerï¼‰

VisionKit æœ€ç»å…¸çš„åŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„æ–‡æ¡£æ‰«æä½“éªŒã€‚

```swift
import VisionKit
import SwiftUI

// UIKit å®ç°
class DocumentScannerViewController: UIViewController {

    func showDocumentScanner() {
        // æ£€æŸ¥è®¾å¤‡æ˜¯å¦æ”¯æŒæ–‡æ¡£æ‰«æ
        guard VNDocumentCameraViewController.isSupported else {
            print("è¯¥è®¾å¤‡ä¸æ”¯æŒæ–‡æ¡£æ‰«æ")
            return
        }

        let scanner = VNDocumentCameraViewController()
        scanner.delegate = self
        present(scanner, animated: true)
    }
}

extension DocumentScannerViewController: VNDocumentCameraViewControllerDelegate {

    // æ‰«æå®Œæˆ
    func documentCameraViewController(_ controller: VNDocumentCameraViewController, didFinishWith scan: VNDocumentCameraScan) {
        controller.dismiss(animated: true)

        // å¤„ç†æ‰«æç»“æœ
        print("æ‰«æäº† \(scan.pageCount) é¡µ")

        // è·å–æ¯ä¸€é¡µçš„å›¾ç‰‡
        for pageIndex in 0..<scan.pageCount {
            let image = scan.imageOfPage(at: pageIndex)
            // ä¿å­˜æˆ–å¤„ç†å›¾ç‰‡
            processScannedImage(image)
        }
    }

    // å–æ¶ˆæ‰«æ
    func documentCameraViewControllerDidCancel(_ controller: VNDocumentCameraViewController) {
        controller.dismiss(animated: true)
        print("ç”¨æˆ·å–æ¶ˆäº†æ‰«æ")
    }

    // æ‰«æå¤±è´¥
    func documentCameraViewController(_ controller: VNDocumentCameraViewController, didFailWithError error: Error) {
        controller.dismiss(animated: true)
        print("æ‰«æå¤±è´¥: \(error.localizedDescription)")
    }

    private func processScannedImage(_ image: UIImage) {
        // å¯ä»¥ä½¿ç”¨ Vision Framework è¿›è¡Œ OCR
        let manager = TextRecognitionManager()
        manager.recognizeText(in: image) { text in
            if let text = text {
                print("è¯†åˆ«åˆ°çš„æ–‡å­—ï¼š\n\(text)")
            }
        }
    }
}

// SwiftUI å°è£…
struct DocumentScannerView: UIViewControllerRepresentable {

    @Binding var scannedImages: [UIImage]
    @Environment(\.presentationMode) var presentationMode

    func makeUIViewController(context: Context) -> VNDocumentCameraViewController {
        let scanner = VNDocumentCameraViewController()
        scanner.delegate = context.coordinator
        return scanner
    }

    func updateUIViewController(_ uiViewController: VNDocumentCameraViewController, context: Context) {}

    func makeCoordinator() -> Coordinator {
        Coordinator(self)
    }

    class Coordinator: NSObject, VNDocumentCameraViewControllerDelegate {
        let parent: DocumentScannerView

        init(_ parent: DocumentScannerView) {
            self.parent = parent
        }

        func documentCameraViewController(_ controller: VNDocumentCameraViewController, didFinishWith scan: VNDocumentCameraScan) {
            var images: [UIImage] = []

            for pageIndex in 0..<scan.pageCount {
                let image = scan.imageOfPage(at: pageIndex)
                images.append(image)
            }

            parent.scannedImages = images
            parent.presentationMode.wrappedValue.dismiss()
        }

        func documentCameraViewControllerDidCancel(_ controller: VNDocumentCameraViewController) {
            parent.presentationMode.wrappedValue.dismiss()
        }

        func documentCameraViewController(_ controller: VNDocumentCameraViewController, didFailWithError error: Error) {
            print("æ‰«æå¤±è´¥: \(error)")
            parent.presentationMode.wrappedValue.dismiss()
        }
    }
}

// SwiftUI ä½¿ç”¨ç¤ºä¾‹
struct ContentView: View {
    @State private var scannedImages: [UIImage] = []
    @State private var showScanner = false

    var body: some View {
        VStack {
            Button("æ‰«ææ–‡æ¡£") {
                showScanner = true
            }

            if !scannedImages.isEmpty {
                Text("å·²æ‰«æ \(scannedImages.count) é¡µ")

                ScrollView {
                    ForEach(scannedImages.indices, id: \.self) { index in
                        Image(uiImage: scannedImages[index])
                            .resizable()
                            .scaledToFit()
                            .frame(maxWidth: .infinity)
                            .padding()
                    }
                }
            }
        }
        .sheet(isPresented: $showScanner) {
            DocumentScannerView(scannedImages: $scannedImages)
        }
    }
}
```

### 2. å®æ—¶æ•°æ®æ‰«æï¼ˆDataScannerViewController - iOS 16+ï¼‰

DataScannerViewController æä¾›å®æ—¶æ‰«ææ–‡æœ¬å’Œæ¡å½¢ç çš„èƒ½åŠ›ã€‚

```swift
import VisionKit
import SwiftUI

@available(iOS 16.0, *)
class LiveDataScannerViewController: UIViewController {

    private var dataScannerVC: DataScannerViewController?

    override func viewDidLoad() {
        super.viewDidLoad()
        setupDataScanner()
    }

    private func setupDataScanner() {
        // æ£€æŸ¥è®¾å¤‡æ˜¯å¦æ”¯æŒ
        guard DataScannerViewController.isSupported,
              DataScannerViewController.isAvailable else {
            print("è®¾å¤‡ä¸æ”¯æŒå®æ—¶æ•°æ®æ‰«æ")
            return
        }

        // é…ç½®è¦è¯†åˆ«çš„æ•°æ®ç±»å‹
        let recognizedDataTypes: Set<DataScannerViewController.RecognizedDataType> = [
            .text(languages: ["zh-Hans", "en-US"]),  // æ–‡æœ¬è¯†åˆ«
            .barcode(symbologies: [.qr, .ean13])     // æ¡å½¢ç è¯†åˆ«
        ]

        // åˆ›å»ºæ‰«æå™¨
        dataScannerVC = DataScannerViewController(
            recognizedDataTypes: recognizedDataTypes,
            qualityLevel: .balanced,              // accurate, balanced, fast
            recognizesMultipleItems: true,        // æ˜¯å¦è¯†åˆ«å¤šä¸ªé¡¹ç›®
            isHighFrameRateTrackingEnabled: true, // é«˜å¸§ç‡è¿½è¸ª
            isPinchToZoomEnabled: true,           // æåˆç¼©æ”¾
            isGuidanceEnabled: true,              // æ˜¾ç¤ºå¼•å¯¼
            isHighlightingEnabled: true           // é«˜äº®è¯†åˆ«ç»“æœ
        )

        dataScannerVC?.delegate = self

        // æ·»åŠ åˆ°è§†å›¾å±‚çº§
        if let scannerVC = dataScannerVC {
            addChild(scannerVC)
            view.addSubview(scannerVC.view)
            scannerVC.view.frame = view.bounds
            scannerVC.didMove(toParent: self)
        }
    }

    override func viewDidAppear(_ animated: Bool) {
        super.viewDidAppear(animated)

        // å¼€å§‹æ‰«æ
        try? dataScannerVC?.startScanning()
    }

    override func viewWillDisappear(_ animated: Bool) {
        super.viewWillDisappear(animated)

        // åœæ­¢æ‰«æ
        dataScannerVC?.stopScanning()
    }
}

@available(iOS 16.0, *)
extension LiveDataScannerViewController: DataScannerViewControllerDelegate {

    // æ£€æµ‹åˆ°æ–°é¡¹ç›®
    func dataScanner(_ dataScanner: DataScannerViewController, didAdd addedItems: [RecognizedItem], allItems: [RecognizedItem]) {

        for item in addedItems {
            switch item {
            case .text(let text):
                print("è¯†åˆ«åˆ°æ–‡å­—: \(text.transcript)")
                handleRecognizedText(text)

            case .barcode(let barcode):
                print("è¯†åˆ«åˆ°æ¡å½¢ç : \(barcode.payloadStringValue ?? "æ— å†…å®¹")")
                handleRecognizedBarcode(barcode)

            @unknown default:
                break
            }
        }
    }

    // é¡¹ç›®æ›´æ–°
    func dataScanner(_ dataScanner: DataScannerViewController, didUpdate updatedItems: [RecognizedItem], allItems: [RecognizedItem]) {
        // å¤„ç†æ›´æ–°çš„é¡¹ç›®
    }

    // é¡¹ç›®ç§»é™¤
    func dataScanner(_ dataScanner: DataScannerViewController, didRemove removedItems: [RecognizedItem], allItems: [RecognizedItem]) {
        // å¤„ç†ç§»é™¤çš„é¡¹ç›®
    }

    // ç”¨æˆ·ç‚¹å‡»è¯†åˆ«é¡¹
    func dataScanner(_ dataScanner: DataScannerViewController, didTapOn item: RecognizedItem) {
        switch item {
        case .text(let text):
            // å¤åˆ¶æ–‡å­—åˆ°å‰ªè´´æ¿
            UIPasteboard.general.string = text.transcript
            print("å·²å¤åˆ¶: \(text.transcript)")

        case .barcode(let barcode):
            // å¤„ç†æ¡å½¢ç ç‚¹å‡»
            if let payload = barcode.payloadStringValue,
               let url = URL(string: payload) {
                UIApplication.shared.open(url)
            }

        @unknown default:
            break
        }
    }

    private func handleRecognizedText(_ text: RecognizedItem.Text) {
        // å¤„ç†è¯†åˆ«åˆ°çš„æ–‡å­—
        let transcript = text.transcript
        let bounds = text.bounds

        print("æ–‡å­—: \(transcript)")
        print("ä½ç½®: \(bounds)")
    }

    private func handleRecognizedBarcode(_ barcode: RecognizedItem.Barcode) {
        // å¤„ç†è¯†åˆ«åˆ°çš„æ¡å½¢ç 
        if let payload = barcode.payloadStringValue {
            print("æ¡å½¢ç å†…å®¹: \(payload)")
            print("ç±»å‹: \(barcode.observation.symbology.rawValue)")
        }
    }
}

// SwiftUI å°è£…
@available(iOS 16.0, *)
struct LiveDataScannerView: UIViewControllerRepresentable {

    @Binding var recognizedText: String
    @Binding var recognizedBarcode: String

    func makeUIViewController(context: Context) -> LiveDataScannerViewController {
        let vc = LiveDataScannerViewController()
        return vc
    }

    func updateUIViewController(_ uiViewController: LiveDataScannerViewController, context: Context) {}
}

// SwiftUI ä½¿ç”¨ç¤ºä¾‹
@available(iOS 16.0, *)
struct LiveScannerContentView: View {
    @State private var recognizedText = ""
    @State private var recognizedBarcode = ""
    @State private var showScanner = false

    var body: some View {
        VStack {
            Button("å¼€å§‹å®æ—¶æ‰«æ") {
                showScanner = true
            }

            if !recognizedText.isEmpty {
                Text("è¯†åˆ«æ–‡å­—: \(recognizedText)")
                    .padding()
            }

            if !recognizedBarcode.isEmpty {
                Text("æ¡å½¢ç : \(recognizedBarcode)")
                    .padding()
            }
        }
        .fullScreenCover(isPresented: $showScanner) {
            LiveDataScannerView(
                recognizedText: $recognizedText,
                recognizedBarcode: $recognizedBarcode
            )
        }
    }
}
```

### 3. Live Textï¼ˆImageAnalysisInteraction - iOS 16+ï¼‰

Live Text å…è®¸ç”¨æˆ·ç›´æ¥ä¸å›¾ç‰‡ä¸­çš„æ–‡å­—äº¤äº’ã€‚

```swift
import VisionKit
import UIKit

@available(iOS 16.0, *)
class LiveTextImageViewController: UIViewController {

    private let imageView = UIImageView()
    private let analyzer = ImageAnalyzer()
    private let interaction = ImageAnalysisInteraction()

    override func viewDidLoad() {
        super.viewDidLoad()
        setupImageView()
        setupLiveText()
    }

    private func setupImageView() {
        imageView.contentMode = .scaleAspectFit
        imageView.isUserInteractionEnabled = true
        view.addSubview(imageView)

        // è®¾ç½®çº¦æŸ
        imageView.translatesAutoresizingMaskIntoConstraints = false
        NSLayoutConstraint.activate([
            imageView.topAnchor.constraint(equalTo: view.topAnchor),
            imageView.leadingAnchor.constraint(equalTo: view.leadingAnchor),
            imageView.trailingAnchor.constraint(equalTo: view.trailingAnchor),
            imageView.bottomAnchor.constraint(equalTo: view.bottomAnchor)
        ])
    }

    private func setupLiveText() {
        // æ·»åŠ äº¤äº’
        imageView.addInteraction(interaction)

        // é…ç½®
        interaction.allowLongPressForDataDetectorsInTextMode = true
        interaction.preferredInteractionTypes = [.automatic]  // .textSelection, .dataDetectors, .imageSubject
    }

    // åˆ†æå›¾ç‰‡
    func analyzeImage(_ image: UIImage) async {
        imageView.image = image

        guard let cgImage = image.cgImage else { return }

        // é…ç½®åˆ†æé€‰é¡¹
        let configuration = ImageAnalyzer.Configuration([
            .text,           // æ–‡å­—è¯†åˆ«
            .machineReadableCode,  // æ¡å½¢ç 
            .visualLookUp    // è§†è§‰æŸ¥æ‰¾ï¼ˆè¯†åˆ«ç‰©ä½“ï¼‰
        ])

        do {
            // æ‰§è¡Œåˆ†æ
            let analysis = try await analyzer.analyze(cgImage, configuration: configuration)

            // è®¾ç½®åˆ†æç»“æœ
            interaction.analysis = analysis
            interaction.preferredInteractionTypes = .automatic

            // æ£€æŸ¥åˆ†æç»“æœ
            if analysis.hasResults(for: .text) {
                print("âœ… å›¾ç‰‡åŒ…å«æ–‡å­—")
            }

            if analysis.hasResults(for: .machineReadableCode) {
                print("âœ… å›¾ç‰‡åŒ…å«æ¡å½¢ç ")
            }

            // è·å–è¯†åˆ«åˆ°çš„æ–‡å­—
            if let transcript = analysis.transcript {
                print("è¯†åˆ«æ–‡å­—: \(transcript)")
            }

        } catch {
            print("åˆ†æå¤±è´¥: \(error)")
        }
    }

    // æå–å›¾ç‰‡ä¸»ä½“ï¼ˆiOS 17+ï¼‰
    @available(iOS 17.0, *)
    func extractSubject() async -> UIImage? {
        guard let analysis = interaction.analysis,
              analysis.hasResults(for: .visualLookUp) else {
            return nil
        }

        do {
            // æå–ä¸»ä½“å›¾åƒ
            let subject = try await interaction.subject

            if let imageData = subject.data(as: .png) {
                return UIImage(data: imageData)
            }
        } catch {
            print("æå–ä¸»ä½“å¤±è´¥: \(error)")
        }

        return nil
    }
}

// SwiftUI å®ç°
@available(iOS 16.0, *)
struct LiveTextImageView: View {
    let image: UIImage
    @State private var analyzedImage: Image?
    @State private var overlayView = ImageAnalysisOverlayView()

    var body: some View {
        ZStack {
            if let analyzedImage = analyzedImage {
                analyzedImage
                    .resizable()
                    .scaledToFit()
            } else {
                Image(uiImage: image)
                    .resizable()
                    .scaledToFit()
            }
        }
        .overlay(
            ImageAnalysisOverlayViewWrapper(overlayView: overlayView)
        )
        .task {
            await analyzeImage()
        }
    }

    private func analyzeImage() async {
        let analyzer = ImageAnalyzer()
        let configuration = ImageAnalyzer.Configuration([.text, .machineReadableCode])

        guard let cgImage = image.cgImage else { return }

        do {
            let analysis = try await analyzer.analyze(cgImage, configuration: configuration)
            overlayView.analysis = analysis
            overlayView.preferredInteractionTypes = .automatic
        } catch {
            print("åˆ†æå¤±è´¥: \(error)")
        }
    }
}

// ImageAnalysisOverlayView çš„ SwiftUI åŒ…è£…å™¨
@available(iOS 16.0, *)
struct ImageAnalysisOverlayViewWrapper: UIViewRepresentable {
    let overlayView: ImageAnalysisOverlayView

    func makeUIView(context: Context) -> ImageAnalysisOverlayView {
        return overlayView
    }

    func updateUIView(_ uiView: ImageAnalysisOverlayView, context: Context) {}
}
```

## å®æˆ˜æ¡ˆä¾‹ï¼šä»¿ CapWords å®ç°

ç»“åˆ VisionKit å’Œ Vision Frameworkï¼Œå®ç°ç±»ä¼¼ CapWords çš„ç‰©ä½“è¯†åˆ«åº”ç”¨ã€‚

```swift
import SwiftUI
import VisionKit
import Vision

@available(iOS 16.0, *)
struct ObjectRecognitionApp: View {
    @State private var showCamera = false
    @State private var capturedImage: UIImage?
    @State private var recognizedObjects: [RecognizedObject] = []
    @State private var isAnalyzing = false

    var body: some View {
        NavigationView {
            VStack {
                if let image = capturedImage {
                    // æ˜¾ç¤ºæ‹æ‘„çš„å›¾ç‰‡
                    Image(uiImage: image)
                        .resizable()
                        .scaledToFit()
                        .frame(maxHeight: 300)
                        .cornerRadius(12)
                        .padding()

                    // æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                    if isAnalyzing {
                        ProgressView("æ­£åœ¨è¯†åˆ«...")
                            .padding()
                    } else if !recognizedObjects.isEmpty {
                        ScrollView {
                            VStack(alignment: .leading, spacing: 16) {
                                ForEach(recognizedObjects) { object in
                                    ObjectCard(object: object)
                                }
                            }
                            .padding()
                        }
                    }
                } else {
                    // ç©ºçŠ¶æ€
                    VStack(spacing: 20) {
                        Image(systemName: "camera.fill")
                            .font(.system(size: 80))
                            .foregroundColor(.gray)

                        Text("æ‹ç…§è¯†åˆ«ç‰©ä½“")
                            .font(.title2)
                            .foregroundColor(.gray)
                    }
                }

                Spacer()

                // æ‹ç…§æŒ‰é’®
                Button(action: {
                    showCamera = true
                }) {
                    HStack {
                        Image(systemName: "camera")
                        Text("æ‹ç…§è¯†åˆ«")
                    }
                    .font(.headline)
                    .foregroundColor(.white)
                    .frame(maxWidth: .infinity)
                    .padding()
                    .background(Color.blue)
                    .cornerRadius(12)
                }
                .padding()
            }
            .navigationTitle("ç‰©ä½“è¯†åˆ«")
            .sheet(isPresented: $showCamera) {
                ImagePicker(image: $capturedImage)
            }
            .onChange(of: capturedImage) { newImage in
                if let image = newImage {
                    Task {
                        await analyzeImage(image)
                    }
                }
            }
        }
    }

    // åˆ†æå›¾ç‰‡
    private func analyzeImage(_ image: UIImage) async {
        isAnalyzing = true
        recognizedObjects = []

        guard let cgImage = image.cgImage else {
            isAnalyzing = false
            return
        }

        // 1. ä½¿ç”¨ Vision è¿›è¡Œç‰©ä½“åˆ†ç±»
        let classifications = await classifyImage(cgImage)

        // 2. æ£€æµ‹æ˜¾è‘—æ€§åŒºåŸŸ
        let salientObjects = await detectSalientObjects(cgImage)

        // 3. ç»„åˆç»“æœ
        recognizedObjects = classifications.prefix(5).map { classification in
            RecognizedObject(
                name: classification.identifier,
                confidence: classification.confidence,
                translation: translateToTargetLanguage(classification.identifier)
            )
        }

        isAnalyzing = false
    }

    // å›¾åƒåˆ†ç±»
    private func classifyImage(_ cgImage: CGImage) async -> [VNClassificationObservation] {
        return await withCheckedContinuation { continuation in
            let request = VNClassifyImageRequest { request, error in
                guard let observations = request.results as? [VNClassificationObservation],
                      error == nil else {
                    continuation.resume(returning: [])
                    return
                }

                let filtered = observations.filter { $0.confidence > 0.3 }
                continuation.resume(returning: filtered)
            }

            let handler = VNImageRequestHandler(cgImage: cgImage)
            try? handler.perform([request])
        }
    }

    // æ£€æµ‹æ˜¾è‘—æ€§å¯¹è±¡
    private func detectSalientObjects(_ cgImage: CGImage) async -> [CGRect] {
        return await withCheckedContinuation { continuation in
            let request = VNGenerateAttentionBasedSaliencyImageRequest { request, error in
                guard let observation = request.results?.first as? VNSaliencyImageObservation,
                      error == nil else {
                    continuation.resume(returning: [])
                    return
                }

                let objects = observation.salientObjects?.map { $0.boundingBox } ?? []
                continuation.resume(returning: objects)
            }

            let handler = VNImageRequestHandler(cgImage: cgImage)
            try? handler.perform([request])
        }
    }

    // ç¿»è¯‘åˆ°ç›®æ ‡è¯­è¨€ï¼ˆç¤ºä¾‹ï¼šè¿™é‡Œéœ€è¦é›†æˆç¿»è¯‘ APIï¼‰
    private func translateToTargetLanguage(_ text: String) -> String {
        // å®é™…åº”ç”¨ä¸­åº”è¯¥è°ƒç”¨ç¿»è¯‘ API
        let translations: [String: String] = [
            "dog": "ç‹— ğŸ•",
            "cat": "çŒ« ğŸ±",
            "car": "æ±½è½¦ ğŸš—",
            "tree": "æ ‘ ğŸŒ³",
            "book": "ä¹¦ ğŸ“š",
            "phone": "æ‰‹æœº ğŸ“±",
            "cup": "æ¯å­ â˜•ï¸"
        ]

        return translations[text.lowercased()] ?? text
    }
}

// è¯†åˆ«å¯¹è±¡æ•°æ®æ¨¡å‹
struct RecognizedObject: Identifiable {
    let id = UUID()
    let name: String
    let confidence: Float
    let translation: String
}

// å¯¹è±¡å¡ç‰‡è§†å›¾
struct ObjectCard: View {
    let object: RecognizedObject

    var body: some View {
        HStack {
            VStack(alignment: .leading, spacing: 8) {
                Text(object.translation)
                    .font(.title2)
                    .fontWeight(.bold)

                Text(object.name)
                    .font(.subheadline)
                    .foregroundColor(.gray)

                ProgressView(value: Double(object.confidence))
                    .progressViewStyle(.linear)

                Text("ç½®ä¿¡åº¦: \(Int(object.confidence * 100))%")
                    .font(.caption)
                    .foregroundColor(.gray)
            }

            Spacer()

            Button(action: {
                speakWord(object.translation)
            }) {
                Image(systemName: "speaker.wave.2.fill")
                    .font(.title2)
                    .foregroundColor(.blue)
            }
        }
        .padding()
        .background(Color(.systemGray6))
        .cornerRadius(12)
    }

    private func speakWord(_ word: String) {
        // å®ç°è¯­éŸ³æ’­æ”¾
        print("æ’­æ”¾å‘éŸ³: \(word)")
    }
}

// å›¾ç‰‡é€‰æ‹©å™¨
struct ImagePicker: UIViewControllerRepresentable {
    @Binding var image: UIImage?
    @Environment(\.presentationMode) var presentationMode

    func makeUIViewController(context: Context) -> UIImagePickerController {
        let picker = UIImagePickerController()
        picker.sourceType = .camera
        picker.delegate = context.coordinator
        return picker
    }

    func updateUIViewController(_ uiViewController: UIImagePickerController, context: Context) {}

    func makeCoordinator() -> Coordinator {
        Coordinator(self)
    }

    class Coordinator: NSObject, UIImagePickerControllerDelegate, UINavigationControllerDelegate {
        let parent: ImagePicker

        init(_ parent: ImagePicker) {
            self.parent = parent
        }

        func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
            if let image = info[.originalImage] as? UIImage {
                parent.image = image
            }
            parent.presentationMode.wrappedValue.dismiss()
        }

        func imagePickerControllerDidCancel(_ picker: UIImagePickerController) {
            parent.presentationMode.wrappedValue.dismiss()
        }
    }
}
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å›¾åƒé¢„å¤„ç†

```swift
// é™ä½å›¾ç‰‡åˆ†è¾¨ç‡ä»¥æé«˜å¤„ç†é€Ÿåº¦
func resizeImage(_ image: UIImage, maxDimension: CGFloat) -> UIImage? {
    let size = image.size
    let scale = min(maxDimension / size.width, maxDimension / size.height)

    if scale >= 1 {
        return image
    }

    let newSize = CGSize(width: size.width * scale, height: size.height * scale)

    UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)
    image.draw(in: CGRect(origin: .zero, size: newSize))
    let resizedImage = UIGraphicsGetImageFromCurrentImageContext()
    UIGraphicsEndImageContext()

    return resizedImage
}
```

### 2. å¼‚æ­¥å¤„ç†

```swift
// ä½¿ç”¨å¼‚æ­¥æ“ä½œé¿å…é˜»å¡ä¸»çº¿ç¨‹
func processImageAsync(_ image: UIImage) async -> [VNClassificationObservation] {
    await withCheckedContinuation { continuation in
        DispatchQueue.global(qos: .userInitiated).async {
            // æ‰§è¡Œè€—æ—¶æ“ä½œ
            let result = self.performVisionRequest(image)
            continuation.resume(returning: result)
        }
    }
}
```

### 3. ç¼“å­˜å’Œå¤ç”¨

```swift
// ç¼“å­˜è¯·æ±‚å¤„ç†å™¨
class VisionRequestManager {
    static let shared = VisionRequestManager()

    private var cachedRequests: [String: VNRequest] = [:]

    func getRequest(for type: String) -> VNRequest? {
        return cachedRequests[type]
    }

    func cacheRequest(_ request: VNRequest, for type: String) {
        cachedRequests[type] = request
    }
}
```

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: è¯†åˆ«å‡†ç¡®ç‡ä½

```swift
// è§£å†³æ–¹æ¡ˆï¼š
// 1. æé«˜å›¾ç‰‡è´¨é‡
// 2. ä½¿ç”¨ accurate æ¨¡å¼
request.recognitionLevel = .accurate

// 3. å¯ç”¨è¯­è¨€æ ¡æ­£
request.usesLanguageCorrection = true

// 4. æŒ‡å®šæ­£ç¡®çš„è¯­è¨€
request.recognitionLanguages = ["zh-Hans", "en-US"]
```

### Q2: å¤„ç†é€Ÿåº¦æ…¢

```swift
// è§£å†³æ–¹æ¡ˆï¼š
// 1. é™ä½å›¾ç‰‡åˆ†è¾¨ç‡
let resizedImage = resizeImage(originalImage, maxDimension: 1024)

// 2. ä½¿ç”¨ fast æ¨¡å¼
request.recognitionLevel = .fast

// 3. é™åˆ¶è¯†åˆ«åŒºåŸŸ
request.regionOfInterest = CGRect(x: 0.2, y: 0.2, width: 0.6, height: 0.6)
```

### Q3: å†…å­˜å ç”¨é«˜

```swift
// è§£å†³æ–¹æ¡ˆï¼š
// 1. åŠæ—¶é‡Šæ”¾èµ„æº
autoreleasepool {
    let handler = VNImageRequestHandler(cgImage: cgImage)
    try? handler.perform([request])
}

// 2. æ‰¹é‡å¤„ç†æ—¶é™åˆ¶å¹¶å‘æ•°
let queue = OperationQueue()
queue.maxConcurrentOperationCount = 2
```

## æœ€ä½³å®è·µæ€»ç»“

### ä½¿ç”¨å»ºè®®

```swift
âœ… VisionKit é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦æ ‡å‡† UI çš„æ–‡æ¡£æ‰«æ
- å®æ—¶ç›¸æœºè¯†åˆ«ï¼ˆæ–‡å­—ã€æ¡å½¢ç ï¼‰
- Live Text åŠŸèƒ½
- å¿«é€ŸåŸå‹å¼€å‘

âœ… Vision Framework é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦ç²¾ç»†æ§åˆ¶è¯†åˆ«æµç¨‹
- æ‰¹é‡å›¾åƒå¤„ç†
- è‡ªå®šä¹‰ Core ML æ¨¡å‹
- å¤æ‚çš„å›¾åƒåˆ†æä»»åŠ¡

âœ… æ··åˆä½¿ç”¨ï¼š
- VisionKit ç”¨äºç”¨æˆ·äº¤äº’
- Vision Framework ç”¨äºåå°å¤„ç†
- ç»“åˆä½¿ç”¨å‘æŒ¥å„è‡ªä¼˜åŠ¿
```

### ä»£ç è§„èŒƒ

```swift
// 1. å§‹ç»ˆæ£€æŸ¥è®¾å¤‡æ”¯æŒ
guard VNDocumentCameraViewController.isSupported else {
    // é™çº§å¤„ç†
    return
}

// 2. å¼‚æ­¥å¤„ç†é¿å…é˜»å¡
Task {
    await processImage(image)
}

// 3. é”™è¯¯å¤„ç†
do {
    try handler.perform([request])
} catch {
    print("å¤„ç†å¤±è´¥: \(error)")
}

// 4. å†…å­˜ç®¡ç†
autoreleasepool {
    // å¤„ç†å¤§é‡å›¾ç‰‡
}
```

## æ€»ç»“

### VisionKit vs Vision Framework

| ç‰¹æ€§ | VisionKit | Vision Framework |
|------|-----------|------------------|
| **æŠ½è±¡å±‚çº§** | é«˜å±‚ UI ç»„ä»¶ | åº•å±‚ç®—æ³•æ¡†æ¶ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­ |
| **çµæ´»æ€§** | â­â­â­ | â­â­â­â­â­ |
| **å®šåˆ¶æ€§** | ä½ | é«˜ |
| **å­¦ä¹ æ›²çº¿** | å¹³ç¼“ | é™¡å³­ |
| **é€‚ç”¨åœºæ™¯** | æ ‡å‡†åŠŸèƒ½å¿«é€Ÿå®ç° | å¤æ‚éœ€æ±‚ç²¾ç»†æ§åˆ¶ |

### æ ¸å¿ƒè¦ç‚¹

1. **VisionKit**
   - æä¾›å®Œæ•´çš„ UI ç»„ä»¶
   - å¿«é€Ÿå®ç°æ ‡å‡†åŠŸèƒ½
   - ç³»ç»Ÿçº§ç”¨æˆ·ä½“éªŒ
   - é€‚åˆå¿«é€Ÿå¼€å‘

2. **Vision Framework**
   - å¼ºå¤§çš„åº•å±‚èƒ½åŠ›
   - é«˜åº¦å¯å®šåˆ¶
   - æ”¯æŒ Core ML
   - é€‚åˆå¤æ‚åœºæ™¯

3. **æœ€ä½³å®è·µ**
   - æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¡†æ¶
   - æ³¨æ„æ€§èƒ½ä¼˜åŒ–
   - åšå¥½é”™è¯¯å¤„ç†
   - å¼‚æ­¥å¤„ç†é¿å…é˜»å¡

é€šè¿‡åˆç†ä½¿ç”¨ VisionKit å’Œ Vision Frameworkï¼Œä½ å¯ä»¥æ„å»ºå‡ºåŠŸèƒ½å¼ºå¤§ã€ä½“éªŒä¼˜ç§€çš„è§†è§‰è¯†åˆ«åº”ç”¨ï¼ğŸš€
